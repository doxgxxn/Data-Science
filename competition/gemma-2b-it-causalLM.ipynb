{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":168178971,"sourceType":"kernelVersion"},{"sourceId":10417,"sourceType":"modelInstanceVersion","modelInstanceId":8385},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Science AI Assistant with Gemma 2b-it","metadata":{}},{"cell_type":"markdown","source":"For the Kaggle competition **\"Google â€“ AI Assistants for Data Tasks with Gemma,\"** I've prepared this AI Assistant that can accomplish the task of **\"Explaining or teaching basic data science concepts,\"** as required by the competition.\n\nThis project is also an occasion to explain how a basic **retrieval-augmented generation (RAG)** system works, by showing the role of the data, which constitutes the backbone of the system, the function of embeddings and distance measures, how to retrieve relevant information for the task of answering a question, and how to process such information by first using a distillation prompt and then assembling the answer required by the user in a meaningful and useful way.\n\nIn this project, the lion's share is done by **Gemma**, the state-of-the-art open LLM model released by Google, in its <U>2b-it implementation, the smallest in terms of parameters</U>. Gemma is not the only Google technology presented in the project because I also make use of **ScaNN** ([ScaNN Github repository](https://github.com/google-research/google-research/tree/master/scann)) for recalling the information. Apart from Gemma, ScaNN, and HuggingFace packages for transformers and embeddings, there are no ready-made solutions such as vector stores or RAG packages. You can actually see how everything works under the hood, and if you like it, reuse it for your own projects.\n\n","metadata":{}},{"cell_type":"markdown","source":"![AI with Gemma](https://th.bing.com/th/id/OIG2.MHcSMMiDt6p95N.mjds0?pid=ImgGn)","metadata":{}},{"cell_type":"markdown","source":"# 1. What is a RAG and how it can help to explain or teach basic data science concepts","metadata":{}},{"cell_type":"markdown","source":"A **Retrieval-Augmented Generation (RAG)** is a solution that improves text generation of a large language model by integrating its answers using some kind of external knowledge retrieval.\n\nHence, it combines a **retriever** to fetch relevant information and a **generator** to produce accurate responses based on this retrieved knowledge. Basically, it is just like first doing a search engine query (the retriever), getting the best answers, and then asking a large language model such as **Gemma** or **Gemini** to process the information (generator) to answer an initial question.","metadata":{}},{"cell_type":"markdown","source":"![High-level RAG architecture](https://raw.githubusercontent.com/lmassaron/useful_stuff/main/High-Level%20RAG%20Architecture_rev2.jpg)","metadata":{}},{"cell_type":"markdown","source":"Such an approach ensures AI models have access to up-to-date and relevant facts, improving the quality and reliability of their generated text, especially in tasks like question-answering where factual accuracy is crucial and LLMs are infamous for sometimes coming up with made-up information (hallucinations).\n\nIn this case, **Google Gemma** seems already quite apt at answering basic questions about data science, but the idea is to further improve its competencies by providing it reliable information about AI, statistics, machine learning, and data science in general.","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting up the necessary stuff","metadata":{}},{"cell_type":"markdown","source":"In the first cell of this notebook, some key packages for the project are installed or updated to the latest version:\n\n1. The first command installs or upgrades the **torch** package quietly, specifying version compatibility for CUDA 11.7 from the PyTorch repository.\n2. The second command installs or upgrades the **transformers** package to version **4.38.2**, a popular library for natural language processing tasks.\n3. The third command installs the **accelerate** package, which is used for optimizing machine learning training pipelines.\n4. The fourth command installs the **bitsandbytes** package from the specified index URL, potentially a custom or private package repository.\n5. The fifth command installs or upgrades the **sentence_transformers** package, known for providing pre-trained models for sentence embeddings.\n6. The sixth command installs or upgrades the **scann** package, likely used for approximate nearest neighbor search implementations.\n7. The seventh command installs or upgrades the **wikipedia-api** package, which provides an interface to interact with Wikipedia data programmatically.\n","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n!pip install -q -U transformers==\"4.38.2\"\n!pip install -q accelerate\n!pip install -q -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U sentence_transformers\n!pip install -q -U scann\n!pip install -q -U wikipedia-api","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:08.272899Z","iopub.execute_input":"2024-03-19T22:48:08.273275Z","iopub.status.idle":"2024-03-19T22:48:21.134296Z","shell.execute_reply.started":"2024-03-19T22:48:08.273246Z","shell.execute_reply":"2024-03-19T22:48:21.127364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next cell, the code uses the **os** module to set some environment variables. The first line sets the **CUDA_VISIBLE_DEVICES** variable to \"0,\" which instructs CUDA-enabled applications to use only the GPU with index 0 for computation, useful for managing GPU resources in multi-GPU systems. The second line sets **TOKENIZERS_PARALLELISM** to \"false,\" disabling parallelism in the Hugging Face Tokenizers library, potentially useful for troubleshooting or ensuring single-threaded execution. These environment variable configurations help control GPU usage and tokenizer behavior within the Python environment where this code is executed.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.137568Z","iopub.status.idle":"2024-03-19T22:48:21.138309Z","shell.execute_reply.started":"2024-03-19T22:48:21.137935Z","shell.execute_reply":"2024-03-19T22:48:21.137963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Moreover, since warnings may occur when using new versions of Python packages (aligning versions is often a task in itself), the following cell imports the **warnings** package and suppresses warnings during this session.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.140225Z","iopub.status.idle":"2024-03-19T22:48:21.140874Z","shell.execute_reply.started":"2024-03-19T22:48:21.14054Z","shell.execute_reply":"2024-03-19T22:48:21.140566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next cell, the notebook loads Python libraries and modules for natural language processing tasks. It also includes libraries like **re** for regular expressions, **numpy** and **pandas** for data manipulation, **tqdm** for progress bars, **scann** for approximate nearest neighbor search, and **wikipediaapi** for accessing Wikipedia content (yes, we are going to use Wikipedia as a knowledge base).\n","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport scann\nimport wikipediaapi\n\nimport torch\n\nimport transformers\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig,\n                         )\nfrom sentence_transformers import SentenceTransformer\nimport bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.142903Z","iopub.status.idle":"2024-03-19T22:48:21.143541Z","shell.execute_reply.started":"2024-03-19T22:48:21.143232Z","shell.execute_reply":"2024-03-19T22:48:21.143258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Proceeding by building blocks","metadata":{}},{"cell_type":"markdown","source":"Before proceeding with the notebook, it is necessary to spend a word about how I will proceed in building the solution in a way that can be clear, easily explainable, and both reusable as well as hackable.","metadata":{}},{"cell_type":"markdown","source":"The AI assistant will simply be a class containing all you need for it to work and with methods for changing some settings (such as the temperature, which corresponds to its creativity, or the impersonated role, which influences how it responds) and for asking questions.\n\nAll the internal functions, however, are external, hence they are easier to present as stand-alone code snippets, easily reusable for different purposes or projects, and easily upgradable or hackable. Because as you change an external function, you immediately change the behavior of the class, without having to reinstantiate it again (it actually takes some time to re-index all the knowledge base, which may prevent some fast experimentation).\n","metadata":{}},{"cell_type":"markdown","source":"Here, as a first piece of code, the next cell presents a function that returns the device where to map the model and the data when working with the PyTorch library (used by the HF packages). It works with a **CPU-based** computer, a **GPU** one, and with a **macOS with MPS**.","metadata":{}},{"cell_type":"code","source":"def define_device():\n    \"\"\"Define the device to be used by PyTorch\"\"\"\n\n    # Get the PyTorch version\n    torch_version = torch.__version__\n\n    # Print the PyTorch version\n    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n\n    # Check if MPS (Multi-Process Service) device is available on MacOS\n    if torch.backends.mps.is_available():\n        # If MPS is available, print a message indicating its usage\n        print(\"using MPS device on MacOS\")\n        # Define the device as MPS\n        defined_device = torch.device(\"mps\")\n    else:\n        # If MPS is not available, determine the device based on GPU availability\n        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # Print a message indicating the selected device\n        print(f\"using {defined_device}\")\n\n    # Return the defined device\n    return defined_device\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.145831Z","iopub.status.idle":"2024-03-19T22:48:21.146541Z","shell.execute_reply.started":"2024-03-19T22:48:21.146191Z","shell.execute_reply":"2024-03-19T22:48:21.146221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next cells, instead, present two functions designed to operate using the **SentenceTransformers** package ([the package home page](https://www.sbert.net/index.html)), that can operate with lists of text and map them into embeddings.\n","metadata":{}},{"cell_type":"markdown","source":"Embeddings, such as those processed by packages like **SentenceTransformers**, are representations of text or sentences in a numerical form that capture their semantic meaning. These embeddings are created by transforming words or sentences into high-dimensional vectors, where similar vectors represent similar meanings.","metadata":{}},{"cell_type":"markdown","source":"In the context of **SentenceTransformers**, these embeddings are generated using models like BERT or XLNet that have been fine-tuned to produce meaningful sentence representations. These embeddings can be used for various tasks like clustering, semantic textual similarity, and information retrieval (in our project we actually need a retrieval function) by comparing the vectors using metrics like cosine similarity.","metadata":{}},{"cell_type":"code","source":"def get_embedding(text, embedding_model):\n    \"\"\"Get embeddings for a given text using the provided embedding model\"\"\"\n    \n    # Encode the text to obtain embeddings using the provided embedding model\n    embedding = embedding_model.encode(text, show_progress_bar=False)\n    \n    # Convert the embeddings to a list of floats and return\n    return embedding.tolist()\n\ndef map2embeddings(data, embedding_model):\n    \"\"\"Map a list of texts to their embeddings using the provided embedding model\"\"\"\n    \n    # Initialize an empty list to store embeddings\n    embeddings = []\n\n    # Iterate over each text in the input data list\n    no_texts = len(data)\n    print(f\"Mapping {no_texts} pieces of information\")\n    for i in tqdm(range(no_texts)):\n        # Get embeddings for the current text using the provided embedding model\n        embeddings.append(get_embedding(data[i], embedding_model))\n    \n    # Return the list of embeddings\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.148579Z","iopub.status.idle":"2024-03-19T22:48:21.149222Z","shell.execute_reply.started":"2024-03-19T22:48:21.148871Z","shell.execute_reply":"2024-03-19T22:48:21.148896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next cell contains a simple function capable of removing artifacts such as tokens, double asterisks, or spaces which sometimes appear in outputs from large language models.\n","metadata":{}},{"cell_type":"code","source":"def clean_text(txt, EOS_TOKEN):\n    \"\"\"Clean text by removing specific tokens and redundant spaces\"\"\"\n    txt = (txt\n           .replace(EOS_TOKEN, \"\") # Replace the end-of-sentence token with an empty string\n           .replace(\"**\", \"\")      # Replace double asterisks with an empty string\n           .replace(\"<pad>\", \"\")   # Replace \"<pad>\" with an empty string\n           .replace(\"  \", \" \")     # Replace double spaces with single spaces\n          ).strip()                # Strip leading and trailing spaces from the text\n    return txt","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.151004Z","iopub.status.idle":"2024-03-19T22:48:21.151639Z","shell.execute_reply.started":"2024-03-19T22:48:21.151315Z","shell.execute_reply":"2024-03-19T22:48:21.15134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following function, instead, simply adds an indefinite article to a role name, something useful to make a prompt nicer and easier to read.\n","metadata":{}},{"cell_type":"code","source":"def add_indefinite_article(role_name):\n    \"\"\"Check if a role name has a determinative adjective before it, and if not, add the correct one\"\"\"\n    \n    # Check if the first word is a determinative adjective\n    determinative_adjectives = [\"a\", \"an\", \"the\"]\n    words = role_name.split()\n    if words[0].lower() not in determinative_adjectives:\n        # Use \"a\" or \"an\" based on the first letter of the role name\n        determinative_adjective = \"an\" if words[0][0].lower() in \"aeiou\" else \"a\"\n        role_name = f\"{determinative_adjective} {role_name}\"\n\n    return role_name","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.153483Z","iopub.status.idle":"2024-03-19T22:48:21.154153Z","shell.execute_reply.started":"2024-03-19T22:48:21.153781Z","shell.execute_reply":"2024-03-19T22:48:21.153806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the previous functions, mostly devoted to processing text for better readability, the next class helps first to load and initialize Gemma by quantizing it to 4-bit, reducing its memory footprint and allowing for faster responses, and then to generate text from it. <u>**Gemma** is the core of our generative functions</u>, making it a crucial element for processing information and returning it to the user in the most usable and useful form.\n","metadata":{}},{"cell_type":"markdown","source":"The `GemmaHF` class serves as a wrapper for the Transformers implementation of Gemma. Upon initialization, it sets up the model and tokenizer using the specified model name and a maximum sequence length for the tokenizer. \n\nIn short, the method `initialize_model` is designed to set up a 4-bit quantized causal language model (LLM) and tokenizer and configure them. It begins by defining the data type for computation as `float16`. Then, it creates a configuration for quantization using the `BitsAndBytesConfig` class with settings for 4-bit quantization. The function loads a pre-trained model (Gemma 2b-it in the project, but you can try also the 7b version) with the specified quantization configuration. It also loads a tokenizer with the selected device mapping and maximum sequence length settings. Finally, the method returns the initialized model and tokenizer, ready for use by our AI assistant.\n\nFinally, its `generate_text` method takes a prompt as input and generates a text using the instantiated tokenizer and model, allowing for customization of parameters such as maximum new tokens and temperature for sampling. Under the hood, it encodes the prompt, generates text based on it, decodes the output into text, and returns a list of generated text results.\n","metadata":{}},{"cell_type":"code","source":"class GemmaHF():\n    \"\"\"Wrapper for the Transformers implementation of Gemma\"\"\"\n    \n    def __init__(self, model_name, max_seq_length=2048):\n        self.model_name = model_name\n        self.max_seq_length = max_seq_length\n        \n        # Initialize the model and tokenizer\n        print(\"\\nInitializing model:\")\n        self.device = define_device()\n        self.model, self.tokenizer = self.initialize_model(self.model_name, self.device, self.max_seq_length)\n        \n    def initialize_model(self, model_name, device, max_seq_length):\n        \"\"\"Initialize a 4-bit quantized causal language model (LLM) and tokenizer with specified settings\"\"\"\n\n        # Define the data type for computation\n        compute_dtype = getattr(torch, \"float16\")\n\n        # Define the configuration for quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n\n        # Load the pre-trained model with quantization configuration\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=device,\n            quantization_config=bnb_config,\n        )\n\n        # Load the tokenizer with specified device and max_seq_length\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            device_map=device,\n            max_seq_length=max_seq_length\n        )\n        \n        # Return the initialized model and tokenizer\n        return model, tokenizer\n    \n    def generate_text(self, prompt, max_new_tokens=2048, temperature=0.0):\n        \"\"\"Generate text using the instantiated tokenizer and model with specified settings\"\"\"\n    \n        # Encode the prompt and convert to PyTorch tensor\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n\n        # Determine if sampling should be performed based on temperature\n        do_sample = True if temperature > 0 else False\n\n        # Generate text based on the input prompt\n        outputs = self.model.generate(**input_ids, \n                                      max_new_tokens=max_new_tokens, \n                                      do_sample=do_sample, \n                                      temperature=temperature\n                                     )\n\n        # Decode the generated output into text\n        results = [self.tokenizer.decode(output) for output in outputs]\n\n        # Return the list of generated text results\n        return results","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.156596Z","iopub.status.idle":"2024-03-19T22:48:21.157229Z","shell.execute_reply.started":"2024-03-19T22:48:21.156894Z","shell.execute_reply":"2024-03-19T22:48:21.156918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here we arrive at the core of the generative function (before we just initialized the generative engine, Gemma).","metadata":{}},{"cell_type":"markdown","source":"The `generate_summary_and_answer` function, generates an answer for a given question using context from a dataset. It embeds the input question (using the `get_embedding` function we previously saw), finds similar contexts in the dataset, extracts relevant context based on similarity indices, generates prompts for summarizing the context and providing an answer, generates summaries and answers using the a generative method from a \"model\" class, which can be a wrapper class containing Gemma implementations based on HF Transformers, Keras, Gemma C++ or any other available. Afterwards, the function cleans the generated summary and answer, and returns the cleaned answer for further processing. This function works as a sequence of steps in order to generate informative responses starting from an input question and some knowledge base data previously provided.\n","metadata":{}},{"cell_type":"markdown","source":"The two-step execution processing the information retrieved from the knowledge base is necessary because extraction based on embedded vectors sometimes returns irrelevant information. It is a problem based on the fact that embeddings are a mapping that has many facets (they are high-dimensional themselves) and that distance measures, and methods for finding what documents or text are most similar to your question, are often approximate for performance reasons resulting sometimes in unexpected retrieved results. First summarizing relevant information, a task that Gemma can execute with prowess, helps in having a shorter, more compact, and surely more relevant context to provide to the further processing by Gemma, which consists of writing an answer to your question.\n","metadata":{}},{"cell_type":"markdown","source":"In this process, temperature, the level of creativity, and the role may result in different answers and also different answering styles. I decided to rely on the \"expert data scientist\" role, but you may decide for the \"ELI5 divulgator\" or the \"verbose scholarly narrator\" (at your own risk XD).\n","metadata":{}},{"cell_type":"markdown","source":"Finally, notice the part of the generative prompt that says: \"If the context doesn't provide any relevant information, answer with <I couldn't find a good match in my knowledge base for your query, hence I answer based on my own knowledge>\". This is partly to prevent the assistant from losing its usefulness and to alert the user regarding the assistant providing peculiar answers when the question is off-topic, too difficult, or lacks sufficient information.\n","metadata":{}},{"cell_type":"code","source":"def generate_summary_and_answer(question, data, searcher, embedding_model, model,\n                                max_new_tokens=2048, temperature=0.4, role=\"expert\"):\n    \"\"\"Generate an answer for a given question using context from a dataset\"\"\"\n    \n    # Embed the input question using the provided embedding model\n    embeded_question = np.array(get_embedding(question, embedding_model)).reshape(1, -1)\n    \n    # Find similar contexts in the dataset based on the embedded question\n    neighbors, distances = searcher.search_batched(embeded_question)\n    \n    # Extract context from the dataset based on the indices of similar contexts\n    context = \" \".join([data[pos] for pos in np.ravel(neighbors)])\n    \n    # Get the end-of-sentence token from the tokenizer\n    try:\n        EOS_TOKEN = model.tokenizer.eos_token\n    except:\n        EOS_TOKEN = \"<eos>\"\n    \n    # Add a determinative adjective to the role\n    role = add_indefinite_article(role)\n    \n    # Generate a prompt for summarizing the context\n    prompt = f\"\"\"\n             Summarize this context: \"{context}\" in order to answer the question \"{question}\" as {role}\\\n             SUMMARY:\n             \"\"\".strip() + EOS_TOKEN\n    \n    # Generate a summary based on the prompt\n    results = model.generate_text(prompt, max_new_tokens, temperature)\n    \n    # Clean the generated summary\n    summary = clean_text(results[0].split(\"SUMMARY:\")[-1], EOS_TOKEN)\n\n    # Generate a prompt for providing an answer\n    prompt = f\"\"\"\n             Here is the context: {summary}\n             Using the relevant information from the context \n             and integrating it with your knowledge,\n             provide an answer as {role} to the question: {question}.\n             If the context doesn't provide\n             any relevant information answer with \n             [I couldn't find a good match in my\n             knowledge base for your question, \n             hence I answer based on my own knowledge] \\\n             ANSWER:\n             \"\"\".strip() + EOS_TOKEN\n\n    # Generate an answer based on the prompt\n    results = model.generate_text(prompt, max_new_tokens, temperature)\n    \n    # Clean the generated answer\n    answer = clean_text(results[0].split(\"ANSWER:\")[-1], EOS_TOKEN)\n\n    # Return the cleaned answer\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.159089Z","iopub.status.idle":"2024-03-19T22:48:21.159711Z","shell.execute_reply.started":"2024-03-19T22:48:21.159387Z","shell.execute_reply":"2024-03-19T22:48:21.159412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Wrapping up everything","metadata":{}},{"cell_type":"markdown","source":"At this point, the next cell wraps all the functions into an `AIAssistant` class.\n","metadata":{}},{"cell_type":"markdown","source":"The `AIAssistant` class impersonates an AI assistant that interacts with users by providing answers based on a given knowledge base (basically a list of texts containing the knowledge).\n\nUpon initialization, the class loads an embedding model, indexes the knowledge base for efficient search, initializes a language model and tokenizer, and builds a searcher for similarity search using the SCANN library. The class includes functions to query the knowledge base, adjust the assistant's temperature (creativity), and define its answering style.\n\n- The `query` function generates and prints an answer to a user query by utilizing the `generate_summary_and_answer` function.\n- The `set_temperature` function allows adjusting the assistant's creativity level, while the `set_role` function defines the answering style of the AI assistant.\n\nThis class wraps all together the functionality of an AI assistant that makes good use of embeddings, powerful language models such as Gemma, and similarity search to provide informative responses to user queries based on a predefined knowledge base.\n","metadata":{}},{"cell_type":"markdown","source":"A few notes about ScaNN. ScaNN is a library developed by Google Research that offers efficient and scalable nearest neighbor search capabilities. It provides advantages over other solutions by utilizing techniques like quantization and Anisotropic Hashing, which enhance search performance.\n\nAnisotropic Hashing is a method used in hashing techniques for multimodal retrieval that involves learning projection functions to produce dimensions with varying lengths or scales. This flexibility in scaling can be beneficial for capturing complex relationships and structures in high-dimensional data, offering improved retrieval performance in scenarios where isotropic methods may not be as effective. You can read everything about this method in the paper:\n\nGuo, Ruiqi, et al. \"Accelerating large-scale inference with anisotropic vector quantization.\" International Conference on Machine Learning. PMLR, 2020. ([Paper Link](https://arxiv.org/abs/1908.10396))\n\nor by browsing the code repository at [https://github.com/google-research/google-research/tree/master/scann](https://github.com/google-research/google-research/tree/master/scann)\n\nWhat is interesting to note is that in my solution I do not use the cosine distance but simply the dot product as suggested by this paper:\n\nSteck, Harald, Chaitanya Ekanadham, and Nathan Kallus. \"Is Cosine-Similarity of Embeddings Really About Similarity?.\" arXiv preprint arXiv:2403.05440 (2024). ([Paper Link](https://arxiv.org/html/2403.05440v1))\n\nAnd it works pretty well!\n","metadata":{}},{"cell_type":"code","source":"\nclass AIAssistant():\n    \"\"\"An AI assistant that interacts with users by providing answers based on a provided knowledge base\"\"\"\n    \n    def __init__(self, gemma_model, embeddings_name=\"thenlper/gte-large\", temperature=0.4, role=\"expert\"):\n        \"\"\"Initialize the AI assistant.\"\"\"\n        # Initialize attributes\n        self.embeddings_name = embeddings_name\n        self.knowledge_base = []\n        self.temperature = temperature\n        self.role = role\n        \n        # Initialize Gemma model (it can be transformer-based or any other)\n        self.gemma_model = gemma_model\n        \n        # Load the embedding model\n        self.embedding_model = SentenceTransformer(self.embeddings_name)\n        \n    def store_knowledge_base(self, knowledge_base):\n        \"\"\"Store the knowledge base\"\"\"\n        self.knowledge_base=knowledge_base\n        \n    def learn_knowledge_base(self, knowledge_base):\n        \"\"\"Store and index the knowledge based to be used by the assistant\"\"\"\n        # Storing the knowledge base\n        self.store_knowledge_base(knowledge_base)\n        \n        # Load and index the knowledge base\n        print(\"Indexing and mapping the knowledge base:\")\n        embeddings = map2embeddings(self.knowledge_base, self.embedding_model)\n        self.embeddings = np.array(embeddings).astype(np.float32)\n        \n        # Instantiate the searcher for similarity search\n        self.index_embeddings()\n        \n    def index_embeddings(self):\n        \"\"\"Index the embeddings using ScaNN \"\"\"\n        self.searcher = (scann.scann_ops_pybind.builder(db=self.embeddings, num_neighbors=10, distance_measure=\"dot_product\")\n                 .tree(num_leaves=min(self.embeddings.shape[0] // 2, 1000), \n                       num_leaves_to_search=100, \n                       training_sample_size=self.embeddings.shape[0])\n                 .score_ah(2, anisotropic_quantization_threshold=0.2)\n                 .reorder(100)\n                 .build()\n           )\n        \n    def query(self, query):\n        \"\"\"Query the knowledge base of the AI assistant.\"\"\"\n        # Generate and print an answer to the query\n        answer = generate_summary_and_answer(query, \n                                             self.knowledge_base, \n                                             self.searcher, \n                                             self.embedding_model, \n                                             self.gemma_model,\n                                             temperature=self.temperature,\n                                             role=self.role)\n        print(answer)\n        \n    def set_temperature(self, temperature):\n        \"\"\"Set the temperature (creativity) of the AI assistant.\"\"\"\n        self.temperature = temperature\n        \n    def set_role(self, role):\n        \"\"\"Define the answering style of the AI assistant.\"\"\"\n        self.role = role\n        \n    def save_embeddings(self, filename=\"embeddings.npy\"):\n        \"\"\"Save the embeddings to disk\"\"\"\n        np.save(filename, self.embeddings)\n        \n    def load_embeddings(self, filename=\"embeddings.npy\"):\n        \"\"\"Load the embeddings from disk and index them\"\"\"\n        self.embeddings = np.load(filename)\n        # Re-instantiate the searcher\n        self.index_embeddings()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.161792Z","iopub.status.idle":"2024-03-19T22:48:21.162447Z","shell.execute_reply.started":"2024-03-19T22:48:21.162113Z","shell.execute_reply":"2024-03-19T22:48:21.162139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Providing the knowledge base from Wikipedia","metadata":{}},{"cell_type":"markdown","source":"In order to provide a **knowledge base** for the AI Assistant to work confidently with data science questions, I decided to retrieve some information from Wikipedia.","metadata":{}},{"cell_type":"markdown","source":"**Why Wikipedia?**\n\nActually, Wikipedia provides a vast and diverse range of information on various topics, making it a rich source for context data. Also, its structured organization allows for easy extraction and processing, also thanks to the wikipediaapi interface.\n","metadata":{}},{"cell_type":"markdown","source":"The following code, apart from the first two functions useful for cleaning the text from tags and formatting, extracts references, such as pages or other Wikipedia categories, using the `extract_wikipedia_pages` function. Then, the `get_wikipedia_pages` function takes care to crawl to all the pages and information related to some initial Wikipedia category or page.\n","metadata":{}},{"cell_type":"code","source":"# Pre-compile the regular expression pattern for better performance\nBRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n\ndef remove_braces_and_content(text):\n    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n    return BRACES_PATTERN.sub('', text)\n\ndef clean_string(input_string):\n    \"\"\"Clean the input string.\"\"\"\n    \n    # Remove extra spaces by splitting the string by spaces and joining back together\n    cleaned_string = ' '.join(input_string.split())\n    \n    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n    \n    # Remove all occurrences of curly braces and their content from the cleaned string\n    cleaned_string = remove_braces_and_content(cleaned_string)\n    \n    # Return the cleaned string\n    return cleaned_string","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.164344Z","iopub.status.idle":"2024-03-19T22:48:21.164947Z","shell.execute_reply.started":"2024-03-19T22:48:21.164637Z","shell.execute_reply":"2024-03-19T22:48:21.164662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_wikipedia_pages(wiki_wiki, category_name):\n    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n    \n    # Get the Wikipedia page corresponding to the provided category name\n    category = wiki_wiki.page(\"Category:\" + category_name)\n    \n    # Initialize an empty list to store page titles\n    pages = []\n    \n    # Check if the category exists\n    if category.exists():\n        # Iterate through each article in the category and append its title to the list\n        for article in category.categorymembers.values():\n            pages.append(article.title)\n    \n    # Return the list of page titles\n    return pages","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.166923Z","iopub.status.idle":"2024-03-19T22:48:21.167543Z","shell.execute_reply.started":"2024-03-19T22:48:21.167237Z","shell.execute_reply":"2024-03-19T22:48:21.167262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_wikipedia_pages(categories):\n    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n    \n    # Create a Wikipedia object\n    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n    \n    # Initialize lists to store explored categories and Wikipedia pages\n    explored_categories = []\n    wikipedia_pages = []\n\n    # Iterate through each category\n    print(\"- Processing Wikipedia categories:\")\n    for category_name in categories:\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Get the Wikipedia page corresponding to the category\n        category = wiki_wiki.page(\"Category:\" + category_name)\n        \n        # Extract Wikipedia pages from the category and extend the list\n        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n        \n        # Add the explored category to the list\n        explored_categories.append(category_name)\n\n    # Extract subcategories and remove duplicate categories\n    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n    \n    # Explore subcategories recursively\n    while categories_to_explore:\n        category_name = categories_to_explore.pop()\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Extract more references from the subcategory\n        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n\n        # Iterate through the references\n        for ref in more_refs:\n            # Check if the reference is a category\n            if \"Category:\" in ref:\n                new_category = ref.replace(\"Category:\", \"\")\n                # Add the new category to the explored categories list\n                if new_category not in explored_categories:\n                    explored_categories.append(new_category)\n            else:\n                # Add the reference to the Wikipedia pages list\n                if ref not in wikipedia_pages:\n                    wikipedia_pages.append(ref)\n\n    # Initialize a list to store extracted texts\n    extracted_texts = []\n    \n    # Iterate through each Wikipedia page\n    print(\"- Processing Wikipedia pages:\")\n    for page_title in tqdm(wikipedia_pages):\n        try:\n            # Make a request to the Wikipedia page\n            page = wiki_wiki.page(page_title)\n\n            # Check if the page summary does not contain certain keywords\n            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n                # Append the page title and summary to the extracted texts list\n                if len(page.summary) > len(page.title):\n                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n\n                # Iterate through the sections in the page\n                for section in page.sections:\n                    # Append the page title and section text to the extracted texts list\n                    if len(section.text) > len(page.title):\n                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n                        \n        except Exception as e:\n            print(f\"Error processing page {page.title}: {e}\")\n                    \n    # Return the extracted texts\n    return extracted_texts","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.169598Z","iopub.status.idle":"2024-03-19T22:48:21.170279Z","shell.execute_reply.started":"2024-03-19T22:48:21.169912Z","shell.execute_reply":"2024-03-19T22:48:21.169936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To develop an AI assistant capable of answering questions about data science, I've chosen to begin with topics such as machine learning, data science, statistics, and deep learning artificial intelligence. As evident from the output, the range of topics it covers is truly impressive, even for a seasoned data scientist!\n","metadata":{}},{"cell_type":"code","source":"categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\nextracted_texts = get_wikipedia_pages(categories)\nprint(\"Found\", len(extracted_texts), \"Wikipedia pages\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.171873Z","iopub.status.idle":"2024-03-19T22:48:21.172498Z","shell.execute_reply.started":"2024-03-19T22:48:21.172191Z","shell.execute_reply":"2024-03-19T22:48:21.172217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a last step, the extracted knowledge base is saved to disk for later usage","metadata":{}},{"cell_type":"code","source":"wikipedia_data_science_kb = pd.DataFrame(extracted_texts, columns=[\"wikipedia_text\"])\nwikipedia_data_science_kb.to_csv(\"wikipedia_data_science_kb.csv\", index=False)\nwikipedia_data_science_kb.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.174285Z","iopub.status.idle":"2024-03-19T22:48:21.174889Z","shell.execute_reply.started":"2024-03-19T22:48:21.174578Z","shell.execute_reply":"2024-03-19T22:48:21.174603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. A test run","metadata":{}},{"cell_type":"markdown","source":"We are now ready to test our AI assistant!\n\nWe instantiate it using the Gemma 2b-it and the gte-large embeddings and provide the extracts from Wikipedia as a knowledge base.\n\nThe Generate Text Embedding (gte) model is a variant of the BERT model developed by Alibaba DAMO Academy. This embedding model is available in three versions (large, base, small) and is specifically designed for English text processing. In comparisons with other embedding models, the gte-large variant demonstrates superior performance in retrieval tasks, but it also needs more storage space for embedding vectors compared to competitors (we do not worry much about that because ScaNN is quite fast for this application).\n\nThe instantiation will take a short while, then you can ask a few questions to the AI assistant.\n\n","metadata":{}},{"cell_type":"code","source":"# Initialize the name of the embeddings and model\nembeddings_name = \"thenlper/gte-large\"\nmodel_name = \"/kaggle/input/gemma/transformers/2b-it/1\"\n\n# Create an instance of AIAssistant with specified parameters\ngemma_ai_assistant = AIAssistant(gemma_model=GemmaHF(model_name), embeddings_name=embeddings_name)\n\n# Map the intended knowledge base to embeddings and index it\ngemma_ai_assistant.learn_knowledge_base(knowledge_base=extracted_texts)\n\n# Save the embeddings to disk (for later use)\ngemma_ai_assistant.save_embeddings()\n\n# Set the temperature (creativity) of the AI assistant and set the role\ngemma_ai_assistant.set_temperature(0.0)\ngemma_ai_assistant.set_role(\"data science expert whose explanations are useful, clear and complete\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.176999Z","iopub.status.idle":"2024-03-19T22:48:21.177623Z","shell.execute_reply.started":"2024-03-19T22:48:21.177314Z","shell.execute_reply":"2024-03-19T22:48:21.17734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start with a warm-up question: \"What is the difference between data science, machine learning, and artificial intelligence?\"\n","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What is the difference between data science, machine learning, and artificial intelligence?\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.184351Z","iopub.status.idle":"2024-03-19T22:48:21.184968Z","shell.execute_reply.started":"2024-03-19T22:48:21.184657Z","shell.execute_reply":"2024-03-19T22:48:21.184682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now a more complicated question, that you may also encounter in a data science interview!\n","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"Explain how linear regression works\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.187482Z","iopub.status.idle":"2024-03-19T22:48:21.188128Z","shell.execute_reply.started":"2024-03-19T22:48:21.187792Z","shell.execute_reply":"2024-03-19T22:48:21.187817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's ask for more complex methods and algorithms, such as decision trees.\n","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What are decision trees, and how do they work in machine learning?\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.189989Z","iopub.status.idle":"2024-03-19T22:48:21.19064Z","shell.execute_reply.started":"2024-03-19T22:48:21.190304Z","shell.execute_reply":"2024-03-19T22:48:21.19033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next question, about cross-validation, is a return to fundamentals.","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What is cross-validation, and why is it used in machine learning?\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.19251Z","iopub.status.idle":"2024-03-19T22:48:21.193143Z","shell.execute_reply.started":"2024-03-19T22:48:21.192806Z","shell.execute_reply":"2024-03-19T22:48:21.192832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, a tricky question on regularization. How will the AI Assistant handle it?","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"Explain the concept of regularization and its importance in preventing overfitting in machine learning models\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.194744Z","iopub.status.idle":"2024-03-19T22:48:21.19631Z","shell.execute_reply.started":"2024-03-19T22:48:21.195842Z","shell.execute_reply":"2024-03-19T22:48:21.195877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Preparing for deploying the model","metadata":{}},{"cell_type":"markdown","source":"In order to deploy the model, you just need the files that we saved and a copy of the functions and classes that we used in this notebook. The procedure is the same, although you don't need to embed again the knowledge base, you just reload the previously calcualted embeddings. However, the previously seen code works speedly with a GPU available.","metadata":{}},{"cell_type":"markdown","source":"If you actually have access only with CPU machine at the inference phase, you can leverage the C++ version of Gemma, which, based on 8-bit switched floating point compressed weights, can offer an adequate speed of text processing. I take the compiled version from another notebook (see https://www.kaggle.com/code/lucamassaron/gemma-cpp for more details on the compiling procedure) from where I copy the Gemma C++ executable. I also allow the exacutable to be executed and install Google SentencePiece, whose libraries are necessary for the executable to work (in particular the libsentencepiece.so library).","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/gemma-cpp/gemma_cpp /kaggle/working/gemma_cpp # Copy compiled Gemma C++\n!chmod +x ./gemma_cpp/gemma # Make Gemma C++ executable\n!conda install -q -c conda-forge sentencepiece -y # Install Google SentencePiece (https://github.com/google/sentencepiece)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:48:21.198557Z","iopub.status.idle":"2024-03-19T22:48:21.199274Z","shell.execute_reply.started":"2024-03-19T22:48:21.198903Z","shell.execute_reply":"2024-03-19T22:48:21.19893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following Python code defines a class named `GemmaCPP`, which works as a wrapper for interacting with the C++ implementation of Gemma (https://github.com/google/gemma.cpp). \n\nThe class has an initializer method that takes four parameters: `gemma_cpp`, `tokenizer`, `compressed_weights`, and `model`. These parameters are used to initialize attributes of the class instance with the same names which will later serve for interacting with the commands for the C++ compiled Gemma. Additionally, the class contains a method named `generate_text`, which takes a prompt as input along with optional args and kwargs (for compatibility with other Gemma implementations). Within this method, a shell command is constructed using the provided prompt and other parameters, formatted appropriately to be executed with the Gemma C++ executable. \n\nThe `subprocess.Popen` function is then called to execute the shell command, capturing the standard output (stdout) and standard error (stderr) streams. The stdout data is decoded from bytes to a string, and if there is any error message in stderr, it is printed out. Finally, the method returns the output text wrapped in a list. This code facilitates the generation of text using Gemma's C++ implementation via Python, allowing the integration between the two languages.","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport re\n\nclass GemmaCPP():\n    \"\"\"Wrapper for the C++ implementation of Gemma\"\"\"\n    \n    def __init__(self, gemma_cpp, tokenizer, compressed_weights, model):\n        self.gemma_cpp = gemma_cpp\n        self.tokenizer = tokenizer\n        self.compressed_weights = compressed_weights\n        self.model = model\n        \n    def eliminate_long_dots(self, input_string):\n        \"\"\"Eliminate long sequences of dots from the input string\"\"\"\n        # Define a regular expression pattern to match sequences of 2 or more dots\n        pattern = r'\\.{2,}'\n\n        # Replace all occurrences of the pattern with a space\n        output_string = re.sub(pattern, ' ', input_string)\n\n        return output_string.strip()\n    \n    def beautify_string(self, input_string):\n        \"\"\"Clean the input string by removing non-letter characters at the beginning\n           and isolated letters at the end after multiple spaces\"\"\"\n        # Remove non-letter characters at the beginning of the string\n        output_string = re.sub(r'^[^a-zA-Z]+', '', input_string.strip())\n\n        # Remove isolated letters at the end of the output string after multiple spaces\n        output_string = re.sub(r'\\s{3,}(.+)\\Z', '', output_string.strip())\n\n        return output_string\n        \n    def generate_text(self, prompt, *args, **kwargs):\n        \"\"\"Generate text using the cpp tokenizer and model\"\"\"\n\n        # Define the shell command\n        prompt = prompt.replace('\"', '').replace(\"'\", \"\")\n        shell_command = f'echo \"{prompt}\" | {gemma_cpp} -- --tokenizer {tokenizer} --compressed_weights {compressed_weights} --model {model} --verbosity 0'\n\n        # Execute the shell command and redirect stdout to the Python script's stdout\n        process = subprocess.Popen(shell_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n        output_text = \"\"\n        reading_block = \"[ Reading prompt ]\"\n        \n        # Communicate with the process and capture stdout \n        for k, char in enumerate( iter(lambda: process.stdout.read(1), b'') ):\n            single_char = char.decode(sys.stdout.encoding)\n            output_text += single_char\n            if len(output_text) % 20 == 0:\n                count_reading_blocks = output_text.count(reading_block)\n                if count_reading_blocks > 1:\n                    break\n                    \n        # Remove long sequences of dots and the reading block, beautify the string\n        output_text = output_text.replace(reading_block, \"\")\n        output_text = self.eliminate_long_dots(output_text)\n        output_text = self.beautify_string(output_text)\n        output_text = prompt + output_text\n        \n        # Return output text\n        return [output_text]","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:18:34.596698Z","iopub.execute_input":"2024-03-19T23:18:34.597339Z","iopub.status.idle":"2024-03-19T23:18:34.614104Z","shell.execute_reply.started":"2024-03-19T23:18:34.597296Z","shell.execute_reply":"2024-03-19T23:18:34.613153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that everything is ready, I can reinstantiate a Gemma AI Assistant based on Gemma C++ and the knowledge base previously extractred and processed.","metadata":{}},{"cell_type":"code","source":"embeddings_name = \"thenlper/gte-large\"\ngemma_cpp = \"./gemma_cpp/gemma\"\ntokenizer = \"/kaggle/input/gemma/gemmacpp/2b-it-sfp/1/tokenizer.spm\"\ncompressed_weights = \"/kaggle/input/gemma/gemmacpp/2b-it-sfp/1/2b-it-sfp.sbs\"\nmodel = \"2b-it\"\n\n# Create an instance of the class AIAssistant based on Gemma C++\ngemma_ai_assistant = AIAssistant(\n    gemma_model=GemmaCPP(gemma_cpp, tokenizer, compressed_weights, model),\n    embeddings_name=embeddings_name\n)\n\n# Loading the previously prepared knowledge base and embeddings\nwikipedia_data_science_kb = pd.read_csv(\"wikipedia_data_science_kb.csv\")\nknowledge_base = wikipedia_data_science_kb.wikipedia_text.tolist()\n\n# Uploading the knowledge base and embeddings to the AI assistant\ngemma_ai_assistant.store_knowledge_base(knowledge_base=knowledge_base)\ngemma_ai_assistant.load_embeddings(filename=\"embeddings.npy\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:18:36.194525Z","iopub.execute_input":"2024-03-19T23:18:36.19515Z","iopub.status.idle":"2024-03-19T23:18:59.622072Z","shell.execute_reply.started":"2024-03-19T23:18:36.195098Z","shell.execute_reply":"2024-03-19T23:18:59.620557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try a new query on machine learning topics and see how it takes to get an answer when only CPUs (Kaggle Notebooks have 4 cores) are working:","metadata":{}},{"cell_type":"code","source":"gemma_ai_assistant.query(\"In short, what are the key differences between gradient boosting and random forests?\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T23:18:59.624374Z","iopub.execute_input":"2024-03-19T23:18:59.624786Z","iopub.status.idle":"2024-03-19T23:30:53.055993Z","shell.execute_reply.started":"2024-03-19T23:18:59.624749Z","shell.execute_reply":"2024-03-19T23:30:53.054454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Conclusions","metadata":{}},{"cell_type":"markdown","source":"It seems that the AI Assistant is working fine and promptly answering questions in a correct and usable way. Using the same approach, the same code could also be used for other tasks of this competition such as:\n\n- Answering common questions about the Python programming language\n- Explaining or teaching concepts from Kaggle competition solution write-ups\n- Answering common questions about the Kaggle platform\n\nAll you need is to prepare the context data by extraction from a website, a dataset, or other sources such as the meta-Kaggle meta.\n\nEnjoy your new AI assistant powered by Gemma 2b-it :-)\n","metadata":{}}]}